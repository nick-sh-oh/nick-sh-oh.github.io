---
layout: about
title: about
permalink: /
subtitle: Researcher @ <a href="https://www.socius.org/">socius</a>

profile:
  align: right
  image: profile_picture_b2c.jpg
  image_circular: false # crops the image to make it circular
  more_info: false

news: false # includes a list of news items
latest_posts: false # includes a list of the newest posts
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---

Hi there! I’m Nick (Seungheon) Oh - a researcher working at the intersection of artificial intelligence and human understanding. My work focuses on how we might conceptualise, build, and justify artificial intelligence in ways that advance our understanding at multiple levels - from technical mechanisms to cognitively plausible frameworks.

My journey from [Politics and Economics](https://www.lse.ac.uk/study-at-lse/undergraduate/degree-programmes-2024/bsc-politics-and-economics) at the [London School of Economics and Political Science](https://www.lse.ac.uk/) to NLP engineering at [Numen Capital](https://numencapital.com/) revealed a fundamental tension in AI development: while deep learning prioritised predictive accuracy (y-hat), my social science training emphasised understanding mechanisms and causation (b-hat). This gap inspired me to found [socius](https://socius.org/) - to bring social science principles of explanation and understanding to AI development.

Today, as researcher at [socius](https://socius.org/), I focus on establishing solid theoretical groundwork for artificial intelligence development and deployment - from developing theoretical frameworks that bridge machine and human cognition (e.g., cognitively plausible AI), to justifying how XAI systems can generate scientific insights about real-world phenomena (e.g., philosophy of science), to analysing how individual components interact to create intelligent behaviour (e.g., mechanistic interpretability).

* Theoretical Foundation: Developing [System 1.5](https://nips.cc/virtual/2024/104306), a framework that bridges human and machine cognition by regulating the interplay between intuitive (System 1) and analytical (System 2) processing in artificial intelligence. This work draws inspiration from how human experts navigate between quick pattern recognition and deliberate analysis.

* Philosophical Foundation: Proposing [Computational Interpretabilism](https://nips.cc/virtual/2024/99151), a philosophical framework that establishes how post-hoc explanations from AI systems can generate justified scientific insights. This work provides epistemological foundations for using explainable AI in scientific discovery.

* Mechanistic Foundation: I’m currently exploring how fundamental computational principles emerge in artificial neural networks through the lens of renormalisation theory, exploring parallels between physical systems and deep learning architectures to better understand how AI systems learn and generalise.

Additionally, to support social scientists in their research, I develop open-source Python libraries that make data more accessible. These include [sentibank](https://github.com/socius-org/sentibank), which provides theory-driven sentiment analysis capabilities, and [RedditHarbor](https://github.com/socius-org/RedditHarbor), which streamlines social media data collection - enabling researchers to focus on understanding phenomena rather than technical implementation.

I believe the future of AI lies not merely in optimising performance metrics, but in developing systems grounded in robust theoretical and philosophical foundations. If this vision resonates with you, I welcome collaboration opportunities - please don't hesitate to reach out!